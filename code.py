import torch 
import numpy as np
import pandas as pd


def Linear_backward(dZ, cache):
    """Implements the linear part of the backward propagation process for a single layer
    Inputs:
        dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
        cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    Output:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b

    """
    A_prev, W, b = cache
    m = A_prev.shape[0]

    dW = torch.mm(dZ.t(), A_prev) / m
    db = torch.sum(dZ, dim=0, keepdim=True) / m
    dA_prev = torch.mm(dZ, W.t())
    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.


    Input:
        dA – post activation gradient of the current layer
        cache – contains both the linear cache and the activations cache
        activation (str): The activation function used ('relu' or 'softmax')
    Output:
        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b, Z = cache  # Unpack cache tuple, where Z is the linear component output before activation

    if activation == "relu":
        acivation_derive=(Z > 0).float()
        dZ = acivation_derive * dA
    elif activation == "softmax":
        dZ = torch.softmax(Z, dim=1)- dA


    dA_prev, dW, db = Linear_backward(dZ, (A_prev, W, b))
    return dA_prev, dW, db


def relu_backward (dA, activation_cache):
    """    
    Implements backward propagation for a ReLU unit

    Input:
        dA – the post-activation gradient
        activation_cache – contains Z (stored during the forward propagation):
    Output:
        dZ – gradient of the cost with respect to Z

    """
    Z = activation_cache
    # Create a mask that is 1 where Z > 0 and 0 elsewhere
    dZ = (Z > 0).float() * dA  # Element-wise multiplication of the mask with dA

    return dZ


def softmax_backward (dA, activation_cache):
    """
    Implements backward propagation for a softmax unit

    Input:
        dA – the post-activation gradient
        activation_cache – contains Z (stored during the forward propagation)
    output:
        dZ – gradient of the cost with respect to Z

    """
    Z = activation_cache
    softmax_output = F.softmax(Z, dim=-1)

    # Compute dZ: assuming dA is already calculated as the difference (p_i - y_i) in the case of cross-entropy loss
    dZ = softmax_output - dA

    return dZ


def	L_model_backward(AL, Y, caches):
    """    Implement the backward propagation process for the entire network.

    Input
        AL - the probabilities vector, the output of the forward propagation (L_model_forward)
        Y - the true labels vector (the "ground truth" - true classifications)
        Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache
    Output:
        Grads - a dictionary with the gradients
                grads["dA" + str(l)] = ... 
                grads["dW" + str(l)] = ...
                grads["db" + str(l)] = ...
    """
    Grads = {}
    L = len(caches)  # the number of layers
    m = AL.shape[0]
    
    # Initialize the backpropagation
    # Derivative of cross-entropy loss with respect to AL if using softmax
    dAL = - (Y / AL) + (1 - Y) / (1 - AL)
    
    # Last layer (softmax and cross-entropy loss)
    current_cache = caches[-1]
    Grads["dA" + str(L)], Grads["dW" + str(L)], Grads["db" + str(L)] = softmax_backward(dAL, current_cache[3])
    
    # Loop over the layers backward
    for l in reversed(range(L-1)):
        # lth layer: Relu backpropagation
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = relu_backward(Grads["dA" + str(l + 2)], current_cache[3])
        Grads["dA" + str(l + 1)] = dA_prev_temp
        Grads["dW" + str(l + 1)] = dW_temp
        Grads["db" + str(l + 1)] = db_temp

    return Grads

def update_parameters(parameters, grads, learning_rate):
    """
    Update parameters using gradient descent.
    
    Parameters:
    parameters (dict): A dictionary containing the parameters of the DNN architecture.
    grads (dict): A dictionary containing the gradients, generated by L_model_backward.
    learning_rate (float): The learning rate used to update the parameters (alpha).
    
    Returns:
    parameters (dict): The updated values of the parameters object provided as input.
    """
    L = len(parameters) // 2  # Number of layers in the neural network
    
    # Update each parameter
    for l in range(1, L + 1):
        parameters["W" + str(l)] -= learning_rate * grads["dW" + str(l)]
        parameters["b" + str(l)] -= learning_rate * grads["db" + str(l)]
    
    return parameters


